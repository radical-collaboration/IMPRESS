{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IMPRESS","text":"<p>Integrated Machine-learning for PRotEin Structures at Scale</p> <p>IMPRESS is an asynchronous framework for managing complex protein design pipelines with adaptive decision-making capabilities. It is built for deploying heterogeneous scientific worflows (mixed CPU/GPU and data sharing) in high-performance computing environments. Using a building-block approach to workflow design, IMPRESS enables high-throughput campaigns based on foundation models like AlphaFold and ESM2 or with custom models requiring runtime training and optimization.</p>"},{"location":"#features","title":"Features","text":"\ud83e\uddec Protein Design Pipelines: Prebuilt and custom workflows \ud83d\udd04 Adaptive Execution: Dynamic pipeline spawning \u26a1 HPC Optimized: High-performance async execution \ud83c\udfaf Flexible Architecture: Standard and user-defined pipelines"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from impress import ImpressBasePipeline, ImpressManager\n\nclass MyPipeline(ImpressBasePipeline):\n    def register_pipeline_tasks(self):\n        @self.auto_register_task()\n        async def analyze(self):\n            return \"echo 'Analyzing sequences'\"\n\n    async def run(self):\n        await self.analyze()\n        await self.invoke_adaptive_step(wait=False)\n\nasync def run_dummy_pipelines():\n\n    manager = ImpressManager(execution_backend=ThreadExecutionBackend({}))\n\n    await manager.start(pipeline_setups=[{'name': 'p1', 'config': {}, \n                                          'type': MyPipeline},\n                                         {'name': 'p2', 'config': {}, \n                                          'type': MyPipeline},\n                                         {'name': 'p3', 'config': {},\n                                          'type': MyPipeline}])\n\nasyncio.run(run_dummy_pipelines())\n</code></pre>"},{"location":"getting-started/adaptive-pipelines/","title":"IMPRESS Adaptive Pipeline - Walkthrough","text":"<p>This documentation walks through the adaptive capabilities of the IMPRESS pipeline framework, focusing specifically on how pipelines can dynamically spawn child pipelines during execution.</p>"},{"location":"getting-started/adaptive-pipelines/#adaptive-pipeline-components","title":"Adaptive Pipeline Components","text":""},{"location":"getting-started/adaptive-pipelines/#1-pipeline-configuration-for-adaptivity","title":"1. Pipeline Configuration for Adaptivity","text":"<p>The <code>DummyProteinPipeline</code> class is configured to support adaptive behavior through several key attributes:</p> <pre><code>def __init__(self, name: str, flow: Any, configs: Dict[str, Any] = {}, **kwargs):\n    self.iter_seqs: str = 'MKFLVLACGT'\n    self.generation: int = configs.get('generation', 1)           # Track generation level\n    self.parent_name: str = configs.get('parent_name', 'root')    # Track parent pipeline\n    self.max_generations: int = configs.get('max_generations', 3) # Limit recursion depth\n    super().__init__(name, flow, **configs, **kwargs)\n</code></pre> <p>Key adaptive attributes: - <code>generation</code>: Tracks the current generation level of the pipeline - <code>parent_name</code>: Maintains lineage information - <code>max_generations</code>: Prevents infinite recursion by setting a maximum depth</p>"},{"location":"getting-started/adaptive-pipelines/#2-adaptive-execution-point","title":"2. Adaptive Execution Point","text":"<p>The adaptive behavior is triggered at a specific point in the pipeline execution:</p> <pre><code>async def run(self) -&gt; None:\n    # ... other pipeline steps ...\n\n    await self.run_adaptive_step(wait=True)  # Critical adaptive execution point\n\n    # ... remaining pipeline steps ...\n</code></pre> <p><code>run_adaptive_step(wait=True)</code>: - This method calls the registered adaptive function - <code>wait=True</code> ensures the current pipeline waits for the adaptive logic to complete - Any child pipelines spawned will be submitted to the manager at this point</p>"},{"location":"getting-started/adaptive-pipelines/#3-adaptive-strategy-function","title":"3. Adaptive Strategy Function","text":"<p>The core adaptive logic is implemented in the <code>adaptive_optimization_strategy</code> function:</p> <pre><code>async def adaptive_optimization_strategy(pipeline: DummyProteinPipeline) -&gt; None:\n    # Decision logic: stop if max generations reached OR random condition (50% chance)\n    if pipeline.generation &gt;= pipeline.max_generations or random.random() &gt;= 0.5:\n        return  # No child pipeline created\n\n    # Generate unique name for child pipeline\n    new_name = f\"{pipeline.name}_g{pipeline.generation + 1}\"\n\n    # Configure child pipeline\n    new_config = {\n        'name': new_name,\n        'type': type(pipeline),                    # Same pipeline class\n        'config': {\n            'generation': pipeline.generation + 1,  # Increment generation\n            'parent_name': pipeline.name,           # Set parent reference\n            'max_generations': pipeline.max_generations,\n        },\n        'adaptive_fn': adaptive_optimization_strategy  # Same adaptive function\n    }\n\n    # Submit child pipeline to manager\n    pipeline.submit_child_pipeline_request(new_config)\n</code></pre> <p>Key adaptive mechanisms:</p> <ol> <li>Conditional spawning: Uses generation limits and random conditions to decide whether to create children</li> <li>Configuration inheritance: Child pipelines inherit configuration with incremented generation</li> <li>Recursive adaptivity: Child pipelines use the same adaptive function, enabling multi-generational spawning</li> <li>Pipeline submission: Uses <code>submit_child_pipeline_request()</code> to register new pipelines with the manager</li> </ol>"},{"location":"getting-started/adaptive-pipelines/#4-manager-setup-with-adaptive-function","title":"4. Manager Setup with Adaptive Function","text":"<p>The pipeline manager is configured to support adaptive behavior:</p> <pre><code>async def run() -&gt; None:\n    execution_backend = await ConcurrentExecutionBackend(ThreadPoolExecutor())\n    manager: ImpressManager = ImpressManager(execution_backend)\n\n    pipeline_setups = [PipelineSetup(name=f'p{i}',\n                                     type=DummyProteinPipeline,\n                                     adaptive_fn=adaptive_optimization_strategy)  # Adaptive function registered\n                       for i in range(1, 4)]\n\n    await manager.start(pipeline_setups=pipeline_setups)\n</code></pre> <p>Critical setup element: - <code>adaptive_fn=adaptive_optimization_strategy</code> registers the adaptive function with each pipeline - The manager handles the lifecycle of dynamically created child pipelines</p>"},{"location":"getting-started/adaptive-pipelines/#adaptive-execution-flow","title":"Adaptive Execution Flow","text":"<ol> <li>Initial pipelines (<code>p1</code>, <code>p2</code>, <code>p3</code>) start execution</li> <li>Each pipeline runs its tasks until reaching <code>run_adaptive_step()</code></li> <li>Adaptive function evaluates conditions (generation limit, random chance)</li> <li>If conditions are met, child pipeline configuration is created</li> <li>Child pipeline is submitted to the manager via <code>submit_child_pipeline_request()</code></li> <li>Manager spawns the child pipeline and adds it to the execution pool</li> <li>Child pipelines repeat the process, potentially creating their own children</li> </ol>"},{"location":"getting-started/adaptive-pipelines/#key-adaptive-features","title":"Key Adaptive Features","text":"<ul> <li>Dynamic pipeline creation: Pipelines can spawn children during runtime</li> <li>Configurable conditions: Adaptive logic can be based on any runtime state</li> <li>Generation tracking: Built-in support for tracking pipeline lineage</li> <li>Recursive adaptivity: Child pipelines can also be adaptive</li> <li>Manager integration: Seamless integration with the IMPRESS execution manager</li> </ul>"},{"location":"getting-started/installation/","title":"\ud83e\uddf0 Installation Guide","text":"<p>This page shows you how to set up your environment to use Impress and run asynchronous pipelines with it.  </p> <p>We recommend using a virtual environment to isolate your dependencies.</p>"},{"location":"getting-started/installation/#step-1-create-a-python-environment","title":"\ud83d\udce6 Step 1: Create a Python Environment","text":"<p>Make sure you have Python 3.8 or newer installed. You can check your Python version with:</p> <pre><code>python3 --version\n</code></pre> <p>We recommend creating a virtual environment to keep dependencies clean.</p>"},{"location":"getting-started/installation/#using-venv","title":"Using <code>venv</code>","text":"<p>On Linux/Mac:</p> <pre><code>python3 -m venv impress-env\nsource impress-env/bin/activate\n</code></pre> <p>On Windows:</p> <pre><code>python -m venv impress-env\nimpress-env\\Scripts\\activate\n</code></pre> <p>Your shell prompt should now show <code>(impress-env)</code> indicating the environment is active.</p>"},{"location":"getting-started/installation/#step-2-install-impress","title":"\ud83d\ude80 Step 2: Install Impress","text":"<p>Install Impress and its required dependencies from PyPI:</p> <pre><code>pip install impress\n</code></pre> <p>Impress uses Radical AsyncFlow as its workflow backend. If not installed automatically, you can install it explicitly:</p> <pre><code>pip install radical.asyncflow\n</code></pre>"},{"location":"getting-started/installation/#step-3-verify-installation","title":"\ud83e\uddea Step 3: Verify Installation","text":"<p>Check that Impress is installed and importable:</p> <pre><code>python -c \"from impress import ImpressManager; print('Impress is installed!')\"\n</code></pre> <p>You should see:</p> <pre><code>Impress is installed!\n</code></pre>"},{"location":"getting-started/installation/#step-4-run-a-sample-pipeline","title":"\ud83e\uddec Step 4: Run a Sample Pipeline","text":"<p>Now you\u2019re ready to write and run your own pipelines!  </p>"},{"location":"getting-started/installation/#deactivate-environment","title":"\ud83d\udd04 Deactivate Environment","text":"<p>When you\u2019re done, you can deactivate your virtual environment:</p> <pre><code>deactivate\n</code></pre> <p>Next time you want to work with Impress, just activate the environment again.</p>"},{"location":"getting-started/installation/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>\u2705 Explore the API reference \u2705 Build your own workflows!</p>"},{"location":"getting-started/quick-start/","title":"Build Asynchronous Protein Pipelines","text":"<p>This tutorial walks you step-by-step through writing and running an asynchronous workflow of multiple protein pipelines using IMPRESS manager.</p> <p>By the end, you will have a working script that runs N pipelines concurrently, each of which executes 3 tasks asynchronously.</p>"},{"location":"getting-started/quick-start/#overview","title":"Overview","text":"<p>We\u2019ll build:</p> <p>\u2705 A custom pipeline class, <code>ProteinPipeline</code>, which represents one protein analysis pipeline. \u2705 A script to start all pipelines asynchronously.</p> <p>You can adapt the number of pipelines (<code>N</code>) and tasks as needed.</p>"},{"location":"getting-started/quick-start/#step-1-import-the-required-libraries","title":"Step 1: Import the Required Libraries","text":"<p>First, we import the libraries we need:</p> <pre><code>import asyncio\n\nfrom impress import PipelineSetup\nfrom impress import ImpressBasePipeline\nfrom impress import ImpressManager\n\nfrom radical.asyncflow import RadicalExecutionBackend\n</code></pre> <p>We use:</p> <p>asyncio \u2014 Python\u2019s built-in asynchronous library.</p> <p>RadicalExecutionBackend \u2014 runs tasks in parallel.</p> <p>ImpressBasePipeline \u2014 base class for defining a pipeline.</p> <p>ImpressManager \u2014 manages and executes multiple pipelines.</p>"},{"location":"getting-started/quick-start/#step-2-define-a-custom-pipeline","title":"Step 2: Define a Custom Pipeline","text":"<p>We now define our custom ProteinPipeline. This simulates a simple workflow operating on dummy protein sequences.</p> <pre><code>class ProteinPipeline(ImpressBasePipeline):\n    def __init__(self, name, flow, configs={}, **kwargs):\n        # Simulated sequence data and scores\n        self.iter_seqs = {f\"protein_{i}\": f\"sequence_{i}\" for i in range(1, 4)}\n        self.current_scores = {f\"protein_{i}\": i * 10 for i in range(1, 4)}\n        self.previous_scores = {f\"protein_{i}\": i * 10 for i in range(1, 4)}\n\n        super().__init__(name, flow, **configs, **kwargs)\n</code></pre> <p>Here we:</p> <p>Initialize some dummy sequences and scores.</p> <p>Call the parent constructor to properly set up the pipeline.</p>"},{"location":"getting-started/quick-start/#22-register-tasks","title":"2.2 Register Tasks","text":"<pre><code>    def register_pipeline_tasks(self):\n        @self.auto_register_task()\n        async def s1(*args, **kwargs):\n            return \"python3 run_homology_search.py\"\n\n        @self.auto_register_task()\n        async def s2(*args, **kwargs):\n            return \"python3 annotate_domains.py\"\n\n        @self.auto_register_task()\n        async def s3(*args, **kwargs):\n            return \"python3 predict_structure.py\"\n</code></pre> <p>Here we define 3 tasks:</p> <p>Each task is registered automatically to the pipeline.</p>"},{"location":"getting-started/quick-start/#23-run-the-pipeline","title":"2.3 Run the Pipeline","text":"<pre><code>async def run_pipeline(self): # The tasks will execute sequentially\n    s1_res = await self.s1() \n    s2_res = await self.s2()\n    s3_res = await self.s3()\n</code></pre> <p>This method controls the execution order: Run s1, then s2, then s3 asynchronously. Print the result of each task along with the pipeline name.</p> <p>Tip</p> <p>You can change the execution order of your tasks by passing the handler of each task (without <code>await</code>) to the other task that depends on it. For Example: to make <code>s3</code> wait for both <code>s1</code> and <code>s2</code> execution, then you can rewrite your function as follows:</p> <pre><code>async def run_pipeline(self): # s1/s2 starts first in parallel and s3 will wait for them\n    s1_fut = self.s1()\n    s2_fut = self.s2()\n    s3_res = await self.s3(s1_fut, s2_fut)\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-create-and-run-multiple-pipelines","title":"Step 3: Create and Run Multiple Pipelines","text":"<p>We now create a function that starts N pipelines at once.</p> <pre><code>async def run():\n    manager = ImpressManager(\n        execution_backend=RadicalExecutionBackend({'resource': 'local.localhost'})\n    )\n\n    # start 3 pipelines in parallel and wait for them to finish\n    await manager.start(\n        pipeline_setups = [\n            PipelineSetup(name='p1', type=ProteinPipeline),\n            PipelineSetup(name='p2', type=ProteinPipeline),\n            PipelineSetup(name='p3', type=ProteinPipeline)]\n    )\n</code></pre> <p>Here:</p> <p>We initialize an ImpressManager with a RadicalExecutionBackend to enable parallel task execution.</p> <p>We call start() and provide a list of pipeline setups, each with a unique name (p1, p2, p3) and our ProteinPipeline class.</p> <p>You can add more pipelines by adding more entries to the list.</p>"},{"location":"getting-started/quick-start/#step-4-run-the-script","title":"Step 4: Run the Script","text":"<p>Finally, add the entry point to run everything with <code>asyncio</code>:</p> <pre><code>if __name__ == \"__main__\":\n    asyncio.run(run_pipeline())\n</code></pre> <p>This starts the event loop and runs all the pipelines concurrently.</p> <p>\ud83d\udcbb Full Code Here is the complete script for convenience:</p> <pre><code>import asyncio\n\nfrom impress import PipelineSetup\nfrom impress import ImpressBasePipeline\nfrom impress import ImpressManager\n\nfrom radical.asyncflow import RadicalExecutionBackend\n\n\nclass ProteinPipeline(ImpressBasePipeline):\n    def __init__(self, name, flow, configs={}, **kwargs):\n        self.iter_seqs = {f\"protein_{i}\": f\"sequence_{i}\" for i in range(1, 4)}\n        self.current_scores = {f\"protein_{i}\": i * 10 for i in range(1, 4)}\n        self.previous_scores = {f\"protein_{i}\": i * 10 for i in range(1, 4)}\n\n        super().__init__(name, flow, **configs, **kwargs)\n\n    def register_pipeline_tasks(self):\n        @self.auto_register_task()\n        async def s1(*args, **kwargs):\n            return \"python3 run_homology_search.py\"\n\n        @self.auto_register_task()\n        async def s2(*args, **kwargs):\n            return \"python3 annotate_domains.py\"\n\n        @self.auto_register_task()\n        async def s3(*args, **kwargs):\n            return \"python3 predict_structure.py\"\n\n    async def run(self):\n        s1_res = await self.s1()\n        s2_res = await self.s2()\n        s3_res = await self.s3()\n\nasync def run_pipeline():\n    manager = ImpressManager(\n        execution_backend=RadicalExecutionBackend({'resource': 'local.localhost'})\n    )\n\n    await manager.start(\n        pipeline_setups = [\n            PipelineSetup(name='p1', type=ProteinPipeline),\n            PipelineSetup(name='p2', type=ProteinPipeline),\n            PipelineSetup(name='p3', type=ProteinPipeline)]\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_pipeline())\n</code></pre> <p>Each pipeline runs its three tasks in order, and all pipelines run concurrently.</p>"},{"location":"pipelines/pre_built-pipelines/","title":"IMPRESS Adaptive Pipeline - Protein Binding HPC Use Case","text":"<p>This documentation walks through a real-world, computationally intensive IMPRESS adaptive pipeline for protein binding analysis that runs on High-Performance Computing (HPC) systems with GPU requirements.</p>"},{"location":"pipelines/pre_built-pipelines/#use-case-overview","title":"Use Case Overview","text":"<p>This example demonstrates adaptive optimization for AlphaFold protein structure analysis where: - Each protein requires at least 1 GPU for processing - Pipelines adaptively spawn children based on protein quality degradation - Underperforming proteins are migrated to new pipeline instances for re-optimization - Runs on HPC infrastructure (Purdue Anvil GPU cluster)</p>"},{"location":"pipelines/pre_built-pipelines/#adaptive-components-breakdown","title":"Adaptive Components Breakdown","text":""},{"location":"pipelines/pre_built-pipelines/#1-adaptive-criteria-function","title":"1. Adaptive Criteria Function","text":"<pre><code>async def adaptive_criteria(current_score: float, previous_score: float) -&gt; bool:\n    \"\"\"\n    Determine if protein quality has degraded requiring pipeline migration.\n    \"\"\"\n    return current_score &gt; previous_score\n</code></pre> <p>Adaptive Logic: - Simple but effective: Higher scores indicate degraded protein quality - Comparison-based: Evaluates current vs previous protein structure scores - Migration trigger: Returns <code>True</code> when quality degrades, triggering protein migration</p>"},{"location":"pipelines/pre_built-pipelines/#2-core-adaptive-decision-function","title":"2. Core Adaptive Decision Function","text":"<p>The main adaptive intelligence is implemented in <code>adaptive_decision()</code>:</p> <pre><code>async def adaptive_decision(pipeline: ProteinBindingPipeline) -&gt; Optional[Dict[str, Any]]:\n    MAX_SUB_PIPELINES: int = 3\n    sub_iter_seqs: Dict[str, str] = {}\n\n    # Read current scores from CSV output\n    file_name = f'af_stats_{pipeline.name}_pass_{pipeline.passes}.csv'\n    with open(file_name) as fd:\n        for line in fd.readlines()[1:]:\n            # Parse protein scores from AlphaFold output\n            name, *_, score_str = line.split(',')\n            protein = name.split('.')[0]\n            pipeline.current_scores[protein] = float(score_str)\n</code></pre> <p>Score Processing: - File-based communication: Reads AlphaFold statistics from CSV files - Dynamic score tracking: Updates current protein quality scores - Real-time evaluation: Processes actual computational results</p>"},{"location":"pipelines/pre_built-pipelines/#3-adaptive-migration-logic","title":"3. Adaptive Migration Logic","text":"<pre><code>    # First pass \u2014 establish baseline\n    if not pipeline.previous_scores:\n        pipeline.logger.pipeline_log('Saving current scores as previous and returning')\n        pipeline.previous_scores = copy.deepcopy(pipeline.current_scores)\n        return\n\n    # Identify proteins that deteriorated\n    sub_iter_seqs = {}\n    for protein, curr_score in pipeline.current_scores.items():\n        if protein not in pipeline.iter_seqs:\n            continue\n\n        decision = await adaptive_criteria(curr_score, pipeline.previous_scores[protein])\n\n        if decision:\n            sub_iter_seqs[protein] = pipeline.iter_seqs.pop(protein)  # Remove from current pipeline\n</code></pre> <p>Migration Decision Process:</p> <ol> <li>Baseline establishment: First pass saves scores as reference</li> <li>Protein evaluation: Each protein is individually assessed</li> <li>Selective migration: Only degraded proteins are moved to child pipelines</li> <li>Pipeline cleanup: Migrated proteins are removed from parent pipeline</li> </ol>"},{"location":"pipelines/pre_built-pipelines/#4-child-pipeline-creation-and-resource-management","title":"4. Child Pipeline Creation and Resource Management","text":"<pre><code>    # Spawn new pipeline for underperforming proteins\n    if sub_iter_seqs and pipeline.sub_order &lt; MAX_SUB_PIPELINES:\n        new_name: str = f\"{pipeline.name}_sub{pipeline.sub_order + 1}\"\n\n        pipeline.set_up_new_pipeline_dirs(new_name)\n\n        # Copy PDB files for migrated proteins\n        for protein in sub_iter_seqs:\n            src = f'{pipeline.output_path_af}/{protein}.pdb'\n            dst = f'{pipeline.base_path}/{new_name}_in/{protein}.pdb'\n            shutil.copyfile(src, dst)\n\n        # Configure child pipeline\n        new_config = {\n            'name': new_name,\n            'type': type(pipeline),\n            'adaptive_fn': adaptive_decision,  # Recursive adaptivity\n            'config': {\n                'passes': pipeline.passes,\n                'iter_seqs': sub_iter_seqs,           # Only degraded proteins\n                'seq_rank': pipeline.seq_rank + 1,\n                'sub_order': pipeline.sub_order + 1,\n                'previous_scores': copy.deepcopy(pipeline.previous_scores),\n            } \n        }\n\n        pipeline.submit_child_pipeline_request(new_config)\n</code></pre> <p>Resource and Data Management: - File system operations: Creates directories and copies PDB files for migrated proteins - Selective data transfer: Only problematic proteins are moved to child pipelines - Configuration inheritance: Child pipelines inherit optimization parameters - Recursive adaptivity: Child pipelines can also spawn their own children</p>"},{"location":"pipelines/pre_built-pipelines/#5-pipeline-lifecycle-management","title":"5. Pipeline Lifecycle Management","text":"<pre><code>        pipeline.finalize()\n\n        if not pipeline.fasta_list_2:\n            pipeline.kill_parent = True\n</code></pre> <p>Lifecycle Control: - Parent finalization: Completes current pipeline processing - Conditional termination: Parent pipeline can terminate if no remaining work - Resource optimization: Prevents idle pipeline instances</p>"},{"location":"pipelines/pre_built-pipelines/#6-hpc-resource-configuration","title":"6. HPC Resource Configuration","text":"<pre><code>async def impress_protein_bind() -&gt; None:\n    manager: ImpressManager = ImpressManager(\n        execution_backend=RadicalExecutionBackend({\n            'gpus': 2,                           # GPU allocation per pipeline\n            'cores': 32,                         # CPU cores per pipeline\n            'runtime': 13 * 60,                  # 13 hours maximum runtime\n            'resource': 'purdue.anvil_gpu'       # HPC cluster specification\n        })\n    )\n\n    pipeline_setups: List[PipelineSetup] = [\n        PipelineSetup(\n            name='p1',\n            type=ProteinBindingPipeline,\n            adaptive_fn=adaptive_decision\n        )\n    ]\n\n    await manager.start(pipeline_setups=pipeline_setups)\n</code></pre> <p>HPC Integration: - GPU allocation: 2 GPUs per pipeline for intensive AlphaFold calculations - Resource specification: 32 CPU cores and 13-hour runtime limits - Cluster targeting: Specifically configured for Purdue Anvil GPU cluster - Scalable architecture: Framework handles resource allocation for child pipelines</p>"},{"location":"pipelines/pre_built-pipelines/#adaptive-execution-flow","title":"Adaptive Execution Flow","text":"<ol> <li>Initial pipeline starts with full protein set on HPC with GPU resources</li> <li>AlphaFold processing generates protein structure quality scores</li> <li>Adaptive evaluation compares current vs previous scores per protein</li> <li>Migration decision identifies degraded proteins for re-optimization</li> <li>Child pipeline creation moves problematic proteins to new pipeline instance</li> <li>Resource allocation HPC system allocates GPUs/CPUs to child pipeline</li> <li>Recursive optimization child pipelines can further adapt and spawn children</li> <li>Resource cleanup completed pipelines release HPC resources</li> </ol>"},{"location":"pipelines/pre_built-pipelines/#key-adaptive-features-for-hpc","title":"Key Adaptive Features for HPC","text":"<ul> <li>Performance-based adaptation: Real computational results drive pipeline decisions</li> <li>Resource-aware scaling: GPU/CPU resources allocated per pipeline instance</li> <li>Data locality: PDB files copied to maintain data proximity</li> <li>Hierarchical optimization: Multi-level pipeline spawning for complex optimization</li> <li>HPC integration: Native support for cluster resource management</li> <li>Fault tolerance: Pipeline termination and resource cleanup mechanisms</li> </ul>"}]}